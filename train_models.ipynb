{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Dict, Literal, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wfdb\n",
    "from torch.nn.parallel import DataParallel as DP\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP  # noqa: F401\n",
    "from torch_ecg.cfg import CFG, DEFAULTS\n",
    "from torch_ecg.utils.misc import str2bool\n",
    "from torch_ecg.utils.utils_nn import default_collate_fn as collate_fn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from cfg import ModelCfg, TrainCfg\n",
    "from data_reader import CODE15, PTBXL, SamiTrop\n",
    "from dataset import CINC2025Dataset\n",
    "from models import CRNN_CINC2025, FM_CINC2025\n",
    "from outputs import CINC2025Outputs\n",
    "from trainer import CINC2025Trainer\n",
    "from utils.samplers import BalancedBatchSampler\n",
    "\n",
    "# sys.path.insert(0, \"/home/wenh06/Jupyter/wenhao/workspace/torch_ecg/\")\n",
    "# sys.path.insert(0, \"/home/wenh06/Jupyter/wenhao/workspace/bib_lookup/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = Path(\"/home/wenh06/Jupyter/Hot-data/cinc2025/\")\n",
    "# db_dir = Path(\"/home/wenh06/Jupyter/Hot-data/cinc2025-test/\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FLAG = False\n",
    "\n",
    "if ModelCfg.torch_dtype == torch.float64:\n",
    "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "    DTYPE = np.float64\n",
    "else:\n",
    "    DTYPE = np.float32\n",
    "\n",
    "CINC2025Dataset.__DEBUG__ = False\n",
    "CRNN_CINC2025.__DEBUG__ = False\n",
    "FM_CINC2025.__DEBUG__ = False\n",
    "CINC2025Trainer.__DEBUG__ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = deepcopy(TrainCfg)\n",
    "train_config.db_dir = db_dir\n",
    "train_config.debug = True\n",
    "\n",
    "train_config.n_epochs = 20\n",
    "train_config.batch_size = 192  # 16G (Tesla T4)\n",
    "# train_config.log_step = 20\n",
    "\n",
    "# for CRNN only, comment if using FM_CINC2025\n",
    "train_config.learning_rate = 3e-4  # 5e-4, 1e-3\n",
    "train_config.lr = train_config.learning_rate\n",
    "train_config.max_lr = 9e-4\n",
    "\n",
    "\n",
    "train_config.lr_scheduler = \"one_cycle\"\n",
    "train_config.early_stopping.patience = train_config.n_epochs // 3\n",
    "\n",
    "train_config.extra_experiment = True\n",
    "train_config.subsample = 0.01  # 0.01, 0.10, 0.5, 1.0\n",
    "\n",
    "# augmentations configurations\n",
    "# TODO: add augmentation configs\n",
    "\n",
    "model_config = deepcopy(ModelCfg)\n",
    "# model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change model architecture\n",
    "print(model_config.crnn.cnn.keys())\n",
    "print(f\"{model_config.crnn.cnn.name=}\")\n",
    "# model_config.crnn.cnn.name = \"tresnetM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.crnn.dem_encoder.mode = \"film\"  # concat\n",
    "model = CRNN_CINC2025(config=model_config.crnn)\n",
    "\n",
    "# model_config.fm.dem_encoder.mode = \"film\"  # concat\n",
    "# model_config.fm.name = \"st-mem\"\n",
    "# model_config.fm.backbone_cache_dir = \"/home/wenh06/Jupyter/models/ST-MEM/st_mem_vit_base_encoder.pth\"\n",
    "# model = FM_CINC2025(config=model_config.fm)\n",
    "\n",
    "model = model.to(device=DEVICE)\n",
    "if isinstance(model, DP):\n",
    "    print(\"model size:\", model.module.module_size, model.module.module_size_)\n",
    "    print(\"Using devices:\", model.device_ids)\n",
    "else:\n",
    "    print(\"model size:\", model.module_size, model.module_size_)\n",
    "    print(\"Using device:\", model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = CINC2025Dataset(train_config, training=True, lazy=True)\n",
    "ds_val = CINC2025Dataset(train_config, training=False, lazy=True)\n",
    "\n",
    "if isinstance(model, FM_CINC2025):\n",
    "    print(\"Using FM_CINC2025 model, adjusting fs and input_len\")\n",
    "    ds_train.reset_resample_fs(model_config.fm.fs[model_config.fm.name], reload=False)\n",
    "    ds_train.reset_input_len(model_config.fm.input_len[model_config.fm.name], reload=False)\n",
    "    ds_val.reset_resample_fs(model_config.fm.fs[model_config.fm.name], reload=False)\n",
    "    ds_val.reset_input_len(model_config.fm.input_len[model_config.fm.name], reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CINC2025Trainer(\n",
    "    model=model,\n",
    "    model_config=model_config,\n",
    "    train_config=train_config,\n",
    "    device=DEVICE,\n",
    "    lazy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._setup_dataloaders(ds_train, ds_val)\n",
    "# trainer._setup_dataloaders(ds_val, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_state_dict = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.log_manager.flush()\n",
    "trainer.log_manager.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer, model, best_model_state_dict, ds_train, ds_val\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [\n",
    "    0.44057780695994747,\n",
    "    0.4556795797767564,\n",
    "    0.45502298095863425,\n",
    "    0.45436638214051217,\n",
    "    0.45370978332239004,\n",
    "    0.44911359159553516,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_ecg._preprocessors import PreprocManager\n",
    "from torch_ecg.utils import make_serializable\n",
    "\n",
    "from helper_code import compute_accuracy, compute_auc, compute_challenge_score, compute_f_measure\n",
    "from team_code import run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    model_path: Union[str, bytes, os.PathLike], model_arch: Literal[\"FM\", \"CRNN\"], verbose: bool = True\n",
    ") -> Dict[str, Union[dict, torch.nn.Module, PreprocManager]]:\n",
    "    \"\"\"Load the trained models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : `path_like`\n",
    "        The path to the trained model.\n",
    "    model_arch : {\"FM\", \"CRNN\"},\n",
    "        Model architecture.\n",
    "    verbose : bool\n",
    "        Whether to display progress information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : Dict[str, Union[dict, nn.Module, PreprocManager]]\n",
    "        The trained model, its training configurations and the preprocessor manager\n",
    "        inferred from the training configurations.\n",
    "\n",
    "    \"\"\"\n",
    "    model_path = Path(model_path).expanduser().resolve()\n",
    "\n",
    "    print(\"Loading the trained model...\")\n",
    "\n",
    "    model_cls = CRNN_CINC2025 if model_arch == \"CRNN\" else FM_CINC2025\n",
    "    model, train_config = model_cls.from_checkpoint(model_path)\n",
    "    model.to(DEVICE)\n",
    "    if isinstance(model, CRNN_CINC2025):\n",
    "        print(\"Using CRNN_CINC2025 model.\")\n",
    "        train_config.fs = model.config.fs\n",
    "        train_config.resample.fs = model.config.fs\n",
    "    elif isinstance(model, FM_CINC2025):\n",
    "        print(\"Using FM_CINC2025 model.\")\n",
    "        train_config.fs = model.config.fs[model.config.name]\n",
    "        train_config.resample.fs = model.config.fs[model.config.name]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model class.\")\n",
    "    ppm_config = CFG(random=False)\n",
    "    ppm_config.update(deepcopy(train_config))\n",
    "    ppm = PreprocManager.from_config(ppm_config)\n",
    "\n",
    "    print(f\"Chagas classification model loaded from {str(model_path)}\")\n",
    "\n",
    "    return {\"model\": model, \"train_config\": train_config, \"preprocessor\": ppm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_expr_dir = Path(\"./saved_models/extra-experiments/\").resolve()\n",
    "ext_expr_models = sorted([str(item) for item in ext_expr_dir.glob(\"BestModel*.pth.tar\")])\n",
    "print(f\"Found {len(ext_expr_models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = deepcopy(TrainCfg)\n",
    "train_config.db_dir = db_dir\n",
    "train_config.debug = True\n",
    "train_config.extra_experiment = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_test = CINC2025Dataset(train_config, training=False, lazy=True, part=\"test\")\n",
    "# df_test = ds_test.reader._df_records.loc[ds_test.records]\n",
    "# labels = df_test.chagas.astype(int).values\n",
    "\n",
    "# print(f\"Found {len(labels)} test samples\")\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "tqdm.pandas(dynamic_ncols=True)\n",
    "\n",
    "PROJECT_DIR = \"./\"\n",
    "with gzip.open(Path(PROJECT_DIR) / \"utils\" / \"code-15-data-split-64-16-20.json.gz\", \"rt\") as f:\n",
    "    code_15_data_split = json.load(f)\n",
    "with gzip.open(Path(PROJECT_DIR) / \"utils\" / \"ptb-xl-data-split-64-16-20.json.gz\", \"rt\") as f:\n",
    "    ptb_xl_data_split = json.load(f)\n",
    "with gzip.open(Path(PROJECT_DIR) / \"utils\" / \"sami-trop-data-split-64-16-20.json.gz\", \"rt\") as f:\n",
    "    sami_trop_data_split = json.load(f)\n",
    "\n",
    "test_records = code_15_data_split[\"test\"] + [item + \"_hr\" for item in ptb_xl_data_split[\"test\"]] + sami_trop_data_split[\"test\"]\n",
    "\n",
    "all_records = find_records(db_dir)\n",
    "\n",
    "df_test = pd.DataFrame(all_records, columns=[\"path\"])\n",
    "df_test[\"path\"] = df_test[\"path\"].progress_apply(lambda s: db_dir / s)\n",
    "df_test[\"record\"] = df_test[\"path\"].progress_apply(lambda s: s.name)\n",
    "df_test = df_test.set_index(\"record\")\n",
    "df_test = df_test.loc[test_records]\n",
    "df_test[\"chagas\"] = df_test[\"path\"].progress_apply(lambda s: load_label(str(s)))\n",
    "\n",
    "labels = df_test[\"chagas\"].values\n",
    "\n",
    "print(f\"Found {len(df_test)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"subsample-(\\d+%)-([A-Z]+)_\"\n",
    "eval_results = []\n",
    "\n",
    "for model_path in tqdm(ext_expr_models[: len(ext_expr_models) // 3], desc=\"Evaluating models\", dynamic_ncols=True):\n",
    "    filename = Path(model_path).name\n",
    "    match = re.search(pattern, filename)\n",
    "\n",
    "    if match:\n",
    "        subsample_ratio = match.group(1)\n",
    "        model_arch = match.group(2)\n",
    "        print(f\"{subsample_ratio=}, {model_arch=}\")\n",
    "        model = load_model(model_path, model_arch)\n",
    "        binary_outputs = np.zeros_like(labels)\n",
    "        probability_outputs = np.zeros_like(labels).astype(float)\n",
    "        for idx, row in tqdm(\n",
    "            enumerate(df_test.itertuples(index=False)), total=len(df_test), dynamic_ncols=True, desc=\"Evaluating test samples\"\n",
    "        ):\n",
    "            binary_output, probability_output = run_model(str(row.path), model, verbose=False)\n",
    "            binary_outputs[idx] = int(binary_output)\n",
    "            probability_outputs[idx] = probability_output\n",
    "        challenge_score = compute_challenge_score(labels, probability_outputs)\n",
    "        auroc, auprc = compute_auc(labels, probability_outputs)\n",
    "        accuracy = compute_accuracy(labels, binary_outputs)\n",
    "        f_measure = compute_f_measure(labels, binary_outputs)\n",
    "\n",
    "        eval_results.append(\n",
    "            make_serializable(\n",
    "                {\n",
    "                    \"challenge_score\": challenge_score,\n",
    "                    \"auroc\": auroc,\n",
    "                    \"auprc\": auprc,\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"f_measure\": f_measure,\n",
    "                    \"subsample_ratio\": subsample_ratio,\n",
    "                    \"model_arch\": model_arch,\n",
    "                }\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"./saved_models/ext-expr-eval-results-1.json\").write_text(json.dumps(eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Results analyze and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[\"resnet-nc\"] = pd.read_csv(\n",
    "    \"./results/TorchECG_02-21_17-38_CRNN_CINC2025_resnet_nature_comm_bottle_neck_adamw_amsgrad_LR_0.0001_BS_128.csv\"\n",
    ")\n",
    "df_results[\"resnet-nc-se\"] = pd.read_csv(\n",
    "    \"./results/TorchECG_02-22_01-33_CRNN_CINC2025_resnet_nature_comm_bottle_neck_se_adamw_amsgrad_LR_0.0001_BS_128.csv\"\n",
    ")\n",
    "df_results[\"tresnet-m\"] = pd.read_csv(\n",
    "    \"./results/TorchECG_02-22_09-09_CRNN_CINC2025_tresnetM_adamw_amsgrad_LR_0.0001_BS_128.csv\"\n",
    ")\n",
    "df_results[\"tresnet-n\"] = pd.read_csv(\n",
    "    \"./results/TorchECG_02-22_01-38_CRNN_CINC2025_tresnetN_adamw_amsgrad_LR_0.0001_BS_128.csv\"\n",
    ")\n",
    "df_results[\"tresnet-f\"] = pd.read_csv(\n",
    "    \"./results/TorchECG_02-22_09-13_CRNN_CINC2025_tresnetF_adamw_amsgrad_LR_0.0001_BS_128.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "part = \"train\"\n",
    "metric = \"challenge_score\"\n",
    "\n",
    "for k, df in df_results.items():\n",
    "    df_metric = df[df.part == part][[metric, \"epoch\"]].dropna()\n",
    "    ax.plot(df_metric.epoch, df_metric[metric], label=k)\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "part = \"val\"\n",
    "metric = \"challenge_score\"\n",
    "\n",
    "for k, df in df_results.items():\n",
    "    df_metric = df[df.part == part][[metric, \"epoch\"]].dropna()\n",
    "    ax.plot(df_metric.epoch, df_metric[metric], label=k)\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "part = \"train\"\n",
    "metric = \"chagas_f_measure\"\n",
    "\n",
    "for k, df in df_results.items():\n",
    "    df_metric = df[df.part == part][[metric, \"epoch\"]].dropna()\n",
    "    ax.plot(df_metric.epoch, df_metric[metric], label=k)\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "part = \"val\"\n",
    "metric = \"chagas_f_measure\"\n",
    "\n",
    "for k, df in df_results.items():\n",
    "    df_metric = df[df.part == part][[metric, \"epoch\"]].dropna()\n",
    "    ax.plot(df_metric.epoch, df_metric[metric], label=k)\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
